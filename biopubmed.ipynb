{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz0FNqYp1bkk",
        "outputId": "be638403-dfea-4a66-9766-413b6a246bd8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import string\n",
        "import logging\n",
        "from Bio import Entrez\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
        "\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "STOP_WORDS = set(stopwords.words('english'))\n",
        "LEMMATIZER = WordNetLemmatizer()\n",
        "\n",
        "def fetch_pubmed_articles(query, max_results, email, api_key=None):\n",
        "    Entrez.email = email\n",
        "    if api_key:\n",
        "        Entrez.api_key = api_key\n",
        "    with Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results) as handle:\n",
        "        search_results = Entrez.read(handle)\n",
        "    id_list = search_results.get(\"IdList\", [])\n",
        "\n",
        "    with Entrez.efetch(db=\"pubmed\", id=id_list, retmode=\"xml\") as handle:\n",
        "        articles = Entrez.read(handle)\n",
        "    return articles\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Aggressive cleaning: lowercasing, punctuation & digit removal, stopword removal, and lemmatizing.\"\"\"\n",
        "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "    text = ''.join(ch for ch in text if not ch.isdigit())\n",
        "    tokens = text.split()\n",
        "    tokens = [LEMMATIZER.lemmatize(word) for word in tokens if word not in STOP_WORDS]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def clean_text_for_llm(text):\n",
        "    \"\"\"Minimal cleaning for LLM training: preserve punctuation and numbers, remove extra whitespace.\"\"\"\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "def save_text(filename, text):\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(text)\n",
        "\n",
        "def save_processed_article(pmid, processed_text, output_dir=\"processed_articles\"):\n",
        "    save_text(os.path.join(output_dir, f\"{pmid}.txt\"), processed_text)\n",
        "\n",
        "def save_llm_cleaned_article(pmid, cleaned_text, output_dir=\"llm_cleaned_articles\"):\n",
        "    save_text(os.path.join(output_dir, f\"{pmid}.txt\"), cleaned_text)\n",
        "\n",
        "def extract_entities(text, nlp_bc5cdr, nlp_jnlpba):\n",
        "    \"\"\"Extract entities using two NER models.\"\"\"\n",
        "    entities = []\n",
        "    doc_bc5cdr = nlp_bc5cdr(text)\n",
        "    for ent in doc_bc5cdr.ents:\n",
        "        if ent.label_ == 'CHEMICAL':\n",
        "            entities.append((ent.text, 'Drug'))\n",
        "        elif ent.label_ == 'DISEASE':\n",
        "            entities.append((ent.text, 'Disease'))\n",
        "    doc_jnlpba = nlp_jnlpba(text)\n",
        "    for ent in doc_jnlpba.ents:\n",
        "        if ent.label_ == 'PROTEIN':\n",
        "            entities.append((ent.text, 'Gene'))\n",
        "    return entities\n",
        "\n",
        "def save_ner_results(ner_results, output_file=\"ner_results.csv\"):\n",
        "    unique_entities = set()  # Track unique (PMID, Entity, Type)\n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['PMID', 'Entity', 'Type'])\n",
        "        for result in ner_results:\n",
        "            pmid = result.get('pmid', 'Unknown')\n",
        "            for entity, entity_type in result.get('entities', []):\n",
        "                key = (pmid, entity, entity_type)\n",
        "                if key not in unique_entities:\n",
        "                    unique_entities.add(key)\n",
        "                    writer.writerow([pmid, entity, entity_type])\n",
        "\n",
        "def extract_relations_from_text(text, extracted_entities, nlp_re):\n",
        "\n",
        "   \n",
        "    relation_verbs = {\n",
        "        \"inhibit\", \"block\", \"suppress\", \"activate\", \"treat\", \"reduce\",\n",
        "        \"upregulate\", \"downregulate\", \"bind\", \"target\", \"modulate\",\n",
        "        \"enhance\", \"promote\", \"interact\", \"associate\", \"regulate\",\n",
        "        \"potentiate\", \"antagonize\", \"induce\", \"overexpress\", \"underexpress\"\n",
        "    }\n",
        "\n",
        "    relations = []\n",
        "    doc = nlp_re(text)\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        sent_text = sent.text\n",
        "        # Check which entities appear in the sentence (case insensitive matching)\n",
        "        present_entities = [\n",
        "            (ent_text, ent_type)\n",
        "            for ent_text, ent_type in extracted_entities\n",
        "            if ent_text.lower() in sent_text.lower()\n",
        "        ]\n",
        "        if len(present_entities) >= 2:\n",
        "            for token in sent:\n",
        "                if token.lemma_.lower() in relation_verbs:\n",
        "                    # For simplicity, use the first two entities found in the sentence.\n",
        "                    entity1, entity2 = present_entities[0], present_entities[1]\n",
        "                    relations.append((entity1[0], token.lemma_, entity2[0], sent_text))\n",
        "                    break  # Stop after finding one relation per sentence.\n",
        "    return relations\n",
        "\n",
        "\n",
        "def save_relation_results(relations, output_file=\"relation_results.csv\"):\n",
        "    \"\"\"Save relation extraction results to a CSV file.\"\"\"\n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['PMID', 'Entity1', 'Relation', 'Entity2', 'Sentence'])\n",
        "        for pmid, entity1, relation, entity2, sentence in relations:\n",
        "            writer.writerow([pmid, entity1, relation, entity2, sentence])\n",
        "\n",
        "def main():\n",
        "    query = input(\"Enter your PubMed query: \")\n",
        "    try:\n",
        "        max_results = int(input(\"Enter number of articles to fetch: \"))\n",
        "    except ValueError:\n",
        "        logging.error(\"Invalid number for max results. Exiting.\")\n",
        "        return\n",
        "\n",
        "    email = input(\"Enter your email address: \")\n",
        "    api_key = input(\"Enter your PubMed API key (optional): \") or None\n",
        "\n",
        "    logging.info(\"Fetching articles from PubMed...\")\n",
        "    try:\n",
        "        articles = fetch_pubmed_articles(query, max_results, email, api_key)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching articles: {e}\")\n",
        "        return\n",
        "\n",
        "    # Load NER models\n",
        "    try:\n",
        "        nlp_bc5cdr = spacy.load(\"en_ner_bc5cdr_md\")\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error loading en_ner_bc5cdr_md model. Ensure it is installed and compatible.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        nlp_jnlpba = spacy.load(\"en_ner_jnlpba_md\")\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error loading en_ner_jnlpba_md model. Ensure it is installed and compatible.\")\n",
        "        return\n",
        "\n",
        "   \n",
        "    try:\n",
        "        nlp_re = spacy.load(\"en_core_web_sm\")\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error loading en_core_web_sm model for relation extraction. Install it with: python -m spacy download en_core_web_sm\")\n",
        "        return\n",
        "\n",
        "    ner_results = []\n",
        "    relation_results = []\n",
        "\n",
        "    for article in articles.get('PubmedArticle', []):\n",
        "        pmid = \"Unknown\"\n",
        "        try:\n",
        "            medline = article.get('MedlineCitation', {})\n",
        "            pmid = medline.get('PMID', 'Unknown')\n",
        "            article_info = medline.get('Article', {})\n",
        "            title = article_info.get('ArticleTitle', '')\n",
        "            abstract = \"\"\n",
        "            if 'Abstract' in article_info:\n",
        "                abs_content = article_info['Abstract']\n",
        "                if isinstance(abs_content, dict) and 'AbstractText' in abs_content:\n",
        "                    abs_text = abs_content['AbstractText']\n",
        "                    abstract = ' '.join(abs_text) if isinstance(abs_text, list) else abs_text\n",
        "            raw_text = f\"{title} {abstract}\"\n",
        "\n",
        "           \n",
        "            llm_cleaned_text = clean_text_for_llm(raw_text)\n",
        "            save_llm_cleaned_article(pmid, llm_cleaned_text)\n",
        "            processed_text = preprocess_text(raw_text)\n",
        "            save_processed_article(pmid, processed_text)\n",
        "\n",
        "            #  entities\n",
        "            entities = extract_entities(raw_text, nlp_bc5cdr, nlp_jnlpba)\n",
        "            ner_results.append({'pmid': pmid, 'entities': entities})\n",
        "\n",
        "            # Extracts relations from raw text using the previously extracted entities\n",
        "            relations = extract_relations_from_text(raw_text, entities, nlp_re)\n",
        "            for rel in relations:\n",
        "                relation_results.append((pmid, rel[0], rel[1], rel[2], rel[3]))\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Skipping article {pmid} due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "\n",
        "    save_ner_results(ner_results)\n",
        "    save_relation_results(relation_results)\n",
        "    logging.info(\"Processing completed. Check the output directories and CSV files.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
